import numpy as np
import matplotlib.pyplot as plt
import warnings

import glob
import os
import pandas as pd
import rubin_sim.maf.db as db
import rubin_sim.maf.metricBundles as metricBundles


def microlensingFOM(
    save_folder,
    resultDbPath,
    metricDataPath,
    figsize=None,
    figure_name="microlensingFOM",
):
    """
    Processes a folder, puts together results for
    discovery/detect metric, Npts metric, and Fisher metric,
    and plots them in the four tE bins of 1 - 10 days,
    10 - 30 days, 30 - 100 days, and 100 - 1000 days.

    Parameters
    ----------
    resultDbPath : string
        Path to the directory storing the result databases
        generated by MAF.
    metricDataPath : string
        Path to the directory storing the npz files
        generated by MAF.
    """

    # get a dictionary of resultDb from given directory
    resultDbs = getResultsDbs(resultDbPath)

    # the following line will be useful if you did not run MAF on all opsims
    runNames = list(resultDbs.keys())

    # retrieve metricBundles for each opsim run and store them in a dictionary
    bundleDicts = {}

    for runName in resultDbs:
        bundleDicts[runName] = bundleDictFromDisk(
            resultDbs[runName], runName, metricDataPath
        )

    # generates results and metric info from default name of file
    results = np.zeros(len(list(bundleDicts.keys())))
    results_compare = []
    run_names = []
    metric_types = []
    min_tEs = np.zeros(len(list(bundleDicts.keys())))
    max_tEs = np.zeros(len(list(bundleDicts.keys())))
    for run in range(len(list(bundleDicts.keys()))):
        npz = np.load(
            resultDbPath + "/" + list(bundleDicts.keys())[run] + ".npz",
            allow_pickle=True,
        )
        relevant_columns = ["metricValues", "mask"]
        df = pd.DataFrame.from_dict({item: npz[item] for item in relevant_columns})
        run_name, metric_type, min_tE, max_tE = parse_tE_run_types(
            list(bundleDicts.keys())[run]
        )
        run_names.append(run_name)
        metric_types.append(metric_type)
        min_tEs[run] = min_tE
        max_tEs[run] = max_tE
        results[run] = getResults(df, metric_type)
        if metric_type == "Npts":
            nan_to_be = np.where(df["metricValues"] >= 10e10)[0]
            df["metricValues"][nan_to_be] = np.nan
        results_compare.append(df["metricValues"])
    run_names = np.array(run_names)
    metric_types = np.array(metric_types)
    results_compare = np.array(results_compare)

    plot_FOM(
        results,
        run_names,
        metric_types,
        min_tEs,
        max_tEs,
        save_folder,
        figure_name,
        figsize=figsize,
    )
    plot_compare(
        results_compare, run_names, metric_types, min_tEs, max_tEs, save_folder
    )

    return


def parse_tE_run_types(name):
    """
    Parses names of MicrolensingMetric file names

    Parameters
    ----------
    name : string
        A MicrolensingMetric file name
    """
    split_name = name.split("MicrolensingMetric")
    run_name = split_name[0][:-1]
    metric_type = split_name[1].split("_")[1]
    min_tE = split_name[1].split("_")[3]
    max_tE = split_name[1].split("_")[4]

    return run_name, metric_type, min_tE, max_tE


def getResults(df, run_type, Fisher_sigmatE_tE_cutoff=0.1):
    """
    Plots the results from the discovery/detect metric, Npts metric,
    and Fisher metric in three sub plots

    Parameters
    ----------
    df : pandas dataframe
        Pandas dataframe of the results npz file
    run_types : numpy array of strings
        Array of strings describing microlensing metric type:
        either 'detect', 'Npts', or 'Fisher' as parsed by the file name
    Fisher_sigmatE_tE_cutoff : float (Default is 0.1)
        Maximum normalized uncertainty in tE (sigmatE/tE) as determined by
        3sigma values of pubished planet microlensing candidates
    """
    total = len(df)
    if run_type == "detect":
        # Fraction of discovered/detected events
        result = len(np.where(df["metricValues"] == 1)[0]) / total
    elif run_type == "Npts":
        # Average number of points per lightcurve
        result = (
            sum(
                df["metricValues"][~np.isnan(df["metricValues"])][
                    df["metricValues"] >= 0
                ][df["metricValues"] <= 10e10]
            )
            / total
        )
    elif run_type == "Fisher":
        # Fraction of events with sigmatE/tE below the cutoff of 0.1
        result = len(np.where(df["metricValues"] < Fisher_sigmatE_tE_cutoff)[0]) / total

    return result


def plot_FOM(
    results, run_names, run_types, min_tE, max_tE, save_folder, figure_name, figsize
):
    """
    Plots the results from the discovery/detect metric, Npts metric,
    and Fisher metric in three sub plots

    Parameters
    ----------
    results : numpy array of floats
        Results from the MicrolensingMetric from getResults() from the
        respective microlensing metric type
    run_names : numpy array of strings
        Array of names of the OpSim run that was used in the metric
    run_types : numpy array of strings
        Array of strings describing microlensing metric type:
        either 'detect', 'Npts', or 'Fisher' as parsed by the file name
    min_tE : numpy array of int/floats
        Array of values describing the minium einstein crossing time (tE)
        as parsed by the file name
    max_tE : numpy array of int/floats
        Array of values describing the maximum einstein crossing time (tE)
        as parsed by the file name
    save_folder : string
        String of folder name to save figure
    figure_name : string
        String of figure name
    figsize : tuple
        Tuple of figure size in inches.
        Default is None, which sets figsize = (25, 30)
    """
    if figsize is None:
        figsize = (25, 30)
    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize=figsize)
    plt.tight_layout()
    plt.subplots_adjust(wspace=0, hspace=0)
    subfig_list = [ax1, ax2, ax3]

    plt.rcdefaults()
    font = {"weight": "heavy", "size": 30}

    plt.rc("font", **font)

    tE_range_list = []
    time_run_names = []
    for i, j in zip(np.unique(min_tE), np.unique(max_tE)):
        idx_in_range = np.where((min_tE >= i) & (max_tE <= j))
        tE_range_list.append(idx_in_range)
        time_run_names.append("tE {}-{} days".format(int(i), int(j)))

    detect_runs_idx = np.where(run_types == "detect")
    Npts_runs_idx = np.where(run_types == "Npts")
    Fisher_runs_idx = np.where(run_types == "Fisher")
    run_type_list = [detect_runs_idx, Npts_runs_idx, Fisher_runs_idx]

    type_run_names = ["Discovery", "Npts", "Fisher"]

    for tE_range in range(len(tE_range_list)):
        for run_type in range(len(run_type_list)):
            # sorted alphabetically according to name of run
            idx_list = list(
                zip(
                    np.intersect1d(tE_range_list[tE_range], run_type_list[run_type]),
                    run_names[
                        np.intersect1d(tE_range_list[tE_range], run_type_list[run_type])
                    ],
                )
            )
            idx_list.sort(key=lambda x: x[1])
            sorted_idxs = np.array([x[0] for x in idx_list])
            subfig_list[run_type].plot(
                results[sorted_idxs],
                run_names[sorted_idxs],
                label=time_run_names[tE_range],
                marker=".",
                markersize=15,
                linewidth=2.5,
            )

    ax3.legend(bbox_to_anchor=(1, 1), fontsize=20)

    plt.tight_layout()
    plt.subplots_adjust(bottom=0.05)
    ax1.set_xlabel("Discovery Efficiency")
    ax2.set_xlabel("Avg Number of Points")

    ax2.set_xscale("log")
    ax3.set_xlabel("Characaterization Efficiency \n ($\sigma_{t_E}/t_E$ < 0.1)")
    plt.savefig(save_folder + "/" + figure_name + ".png", bbox_inches="tight")

    return


def plot_compare(
    results, run_names, run_types, min_tE, max_tE, save_folder, npts_required=10
):
    """
    Plots confusion matrix type plots comparing fraction detected, characterized (via Fisher),
    and fraction of events with at least npts_required points within 2 tE

    Parameters
    ----------
    results : numpy array of floats
        Results from the MicrolensingMetric from getResults() from the
        respective microlensing metric type
    run_names : numpy array of strings
        Array of names of the OpSim run that was used in the metric
    run_types : numpy array of strings
        Array of strings describing microlensing metric type:
        either 'detect', 'Npts', or 'Fisher' as parsed by the file name
    min_tE : numpy array of int/floats
        Array of values describing the minium einstein crossing time (tE)
        as parsed by the file name
    max_tE : numpy array of int/floats
        Array of values describing the maximum einstein crossing time (tE)
        as parsed by the file name
    save_folder : string
        String of folder name to save figures
    npts_required : int (Default is 10).
        Number of poitns within 2tE required for the number of points fraction.
    """

    plt.rcdefaults()
    font = {"weight": "heavy", "size": 20}

    plt.rc("font", **font)

    tE_range_list = []
    time_run_names = []
    for i, j in zip(np.unique(min_tE), np.unique(max_tE)):
        idx_in_range = np.where((min_tE >= i) & (max_tE <= j))
        tE_range_list.append(idx_in_range)
        time_run_names.append("tE {}-{} days".format(int(i), int(j)))

    detect_runs_idx = np.where(run_types == "detect")
    Npts_runs_idx = np.where(run_types == "Npts")
    Fisher_runs_idx = np.where(run_types == "Fisher")
    run_type_list = [detect_runs_idx, Npts_runs_idx, Fisher_runs_idx]

    run_name_list = np.unique(run_names)

    type_run_names = ["Discovery", "Npts", "Fisher"]

    for tE_range in range(len(tE_range_list)):
        for run_name in run_name_list:
            run_name_idxs = np.where(run_names == run_name)
            tE_run_name_interesct = np.intersect1d(
                tE_range_list[tE_range], run_name_idxs
            )
            detect_results = results[
                np.intersect1d(tE_run_name_interesct, detect_runs_idx)
            ]
            npts_results = results[np.intersect1d(tE_run_name_interesct, Npts_runs_idx)]
            fisher_results = results[
                np.intersect1d(tE_run_name_interesct, Fisher_runs_idx)
            ]

            detected_fisher_comparison_matrix = detected_fisher_comparison(
                fisher_results, detect_results
            )
            fisher_npts_comparison_matrix = fisher_npts_comparison(
                fisher_results, npts_results
            )
            detected_npts_comparison_matrix = detected_npts_comparison(
                detect_results, npts_results
            )

            confusion_matrix_plot(
                detected_fisher_comparison_matrix,
                "Discovered",
                "Characterized",
                run_name,
                time_run_names[tE_range],
                save_folder,
            )
            confusion_matrix_plot(
                fisher_npts_comparison_matrix,
                "More than {} Points".format(npts_required),
                "Characterized",
                run_name,
                time_run_names[tE_range],
                save_folder,
            )
            confusion_matrix_plot(
                detected_npts_comparison_matrix,
                "More than {} Points".format(npts_required),
                "Detected",
                run_name,
                time_run_names[tE_range],
                save_folder,
            )

    return


def confusion_matrix_plot(
    comparison_matrix, xlabel, ylabel, run_name, tE_range, save_folder
):
    """
    Plots a confusion matrix type plot comparing two metric types.

    Parameters
    ----------
    comparison_matrix : numpy array
        Array comparing two metric types (A and B) with the following shape:
        [[(Yes A and Yes B), (Yes A and No B)], [(No A and Yes B), (No A and No B)]]
        where Yes A and Yes B are the number of events that pass both the A and B criteria.
    xlabel : string
        Sring of xlabel (also used in file name of figure)
    ylabel : string
        Sring of ylabel (also used in file name of figure)
    run_name : string
        Name of the OpSim run that was used in the metric
        (used in labels and file name)
    tE_range : string
        String of the range of the tE (used in labels and file name)
    save_folder : string
        String of folder name to save figures
    """

    fig, ax = plt.subplots(figsize=(5, 5))

    ax.matshow(comparison_matrix, cmap=plt.cm.Blues, alpha=0.3)
    for i in range(len(comparison_matrix[0])):
        for j in range(len(comparison_matrix[1])):
            ax.text(
                x=j,
                y=i,
                s="{}".format(comparison_matrix[i, j]),
                va="center",
                ha="center",
                size="medium",
            )

    ax.set(ylabel=ylabel, xlabel=xlabel, title=run_name + "\n" + tE_range + "\n")
    ax.set_xticklabels([np.nan, "Yes", "No"])
    ax.set_yticklabels([np.nan, "Yes", "No"])
    plt.tight_layout()
    plt.savefig(
        save_folder + "/{}_{}_{}_{}.png".format(run_name, tE_range, ylabel, xlabel)
    )
    plt.show()
    plt.close()
    return


def detected_fisher_comparison(
    fisher_results, detect_results, Fisher_sigmatE_tE_cutoff=0.1
):
    """
    Returns an array of the following form where
    A = fisher criteria and B = detection criteria:
    [[(Yes A and Yes B), (Yes A and No B)], [(No A and Yes B), (No A and No B)]]
    where Yes A and Yes B are the number of events that pass both the A and B criteria.

    Parameters
    ----------
    fisher_results : numpy array
        Array of results from running the Fisher metric of the microlensing metric
    detect_results : numpy array
        Array of results from running the detect metric of the microlensing metric
    Fisher_sigmatE_tE_cutoff : float (Default is 0.1)
        Maximum normalized uncertainty in tE (sigmatE/tE) as determined by
        3sigma values of pubished planet microlensing candidates
    """
    char_detect = np.where(
        (fisher_results < Fisher_sigmatE_tE_cutoff) & (detect_results == 1)
    )[0]
    char_ndetect = np.where(
        (fisher_results < Fisher_sigmatE_tE_cutoff) & (detect_results == 0)
    )[0]
    nchar_detect = np.where(
        (fisher_results > Fisher_sigmatE_tE_cutoff) & (detect_results == 1)
    )[0]
    nchar_ndetect = np.where(
        (fisher_results > Fisher_sigmatE_tE_cutoff) & (detect_results == 0)
    )[0]
    return np.array(
        [[len(char_detect), len(char_ndetect)], [len(nchar_detect), len(nchar_ndetect)]]
    )


def fisher_npts_comparison(
    fisher_results, npts_results, npts_required=10, Fisher_sigmatE_tE_cutoff=0.1
):
    """
    Returns an array of the following form where
    A = fisher criteria and B = npts criteria:
    [[(Yes A and Yes B), (Yes A and No B)], [(No A and Yes B), (No A and No B)]]
    where Yes A and Yes B are the number of events that pass both the A and B criteria.

    Parameters
    ----------
    fisher_results : numpy array
        Array of results from running the Fisher metric of the microlensing metric
    npts_results : numpy array
        Array of results from running the Npts metric of the microlensing metric
    npts_required : int (Default is 10).
        Number of poitns within 2tE required for the number of points fraction.
    Fisher_sigmatE_tE_cutoff : float (Default is 0.1)
        Maximum normalized uncertainty in tE (sigmatE/tE) as determined by
        3sigma values of pubished planet microlensing candidates
    """
    char_npts = np.where(
        (fisher_results < Fisher_sigmatE_tE_cutoff) & (npts_results > npts_required)
    )[0]
    char_nnpts = np.where(
        (fisher_results < Fisher_sigmatE_tE_cutoff) & (npts_results < npts_required)
    )[0]
    nchar_npts = np.where(
        (fisher_results > Fisher_sigmatE_tE_cutoff) & (npts_results > npts_required)
    )[0]
    nchar_nnpts = np.where(
        (fisher_results > Fisher_sigmatE_tE_cutoff) & (npts_results < npts_required)
    )[0]
    return np.array(
        [[len(char_npts), len(char_nnpts)], [len(nchar_npts), len(nchar_nnpts)]]
    )


def detected_npts_comparison(detect_results, npts_results, npts_required=10):
    """
    Returns an array of the following form where
    A = detect criteria and B = npts criteria:
    [[(Yes A and Yes B), (Yes A and No B)], [(No A and Yes B), (No A and No B)]]
    where Yes A and Yes B are the number of events that pass both the A and B criteria.

    Parameters
    ----------
    detect_results : numpy array
        Array of results from running the detect metric of the microlensing metric
    npts_results : numpy array
        Array of results from running the Npts metric of the microlensing metric
    npts_required : int (Default is 10).
        Number of poitns within 2tE required for the number of points fraction.
    """
    detect_npts = np.where((detect_results == 1) & (npts_results > npts_required))[0]
    detect_nnpts = np.where((detect_results == 1) & (npts_results < npts_required))[0]
    ndetect_npts = np.where((detect_results == 0) & (npts_results > npts_required))[0]
    ndetect_nnpts = np.where((detect_results == 0) & (npts_results < npts_required))[0]
    return np.array(
        [[len(detect_npts), len(detect_nnpts)], [len(ndetect_npts), len(ndetect_nnpts)]]
    )


def getResultsDbs(resultDbPath):
    """
    Create a dictionary of resultDb from resultDb files
    via PCW Hackathan 2020 Resources

    Args:
        resultDbPath(str): Path to the directory storing the result databases
            generated by MAF.

    Returns:
        resultDbs(dict): A dictionary containing the ResultDb objects
            reconstructed from result databases in the provided directory.
    """

    resultDbs = {}
    resultDbList = glob.glob(os.path.join(resultDbPath, "*_result.db"))
    for resultDb in resultDbList:
        runName = os.path.basename(resultDb).rsplit("_", 1)[0]
        resultDb = db.ResultsDb(database=resultDb)

        # Don't add empty results.db file,
        if len(resultDb.getAllMetricIds()) > 0:
            resultDbs[runName] = resultDb

    return resultDbs


def bundleDictFromDisk(resultDb, runName, metricDataPath):
    """
    Load metric data from disk and import them into metricBundles.
    via PCW Hackathan 2020 Resources

    Args:
        resultDb(dict): A ResultDb object.
        runName(str): The name of the opsim database that the metrics stored in
            resultDb was evaluated on.
        metricDataPath(str): The path to the directory where the metric data
            (.npz files) is stored.

    Returns:
        bundleDict(dict): A dictionary of metricBundles reconstructed from data
            stored on disk, the keys designate metric names.
    """

    bundleDict = {}
    displayInfo = resultDb.getMetricDisplayInfo()
    for item in displayInfo:
        metricName = item["metricName"]
        metricFileName = item["metricDataFile"]
        metricId = item["metricId"]
        newbundle = metricBundles.createEmptyMetricBundle()
        newbundle.read(os.path.join(metricDataPath, metricFileName))
        newbundle.setRunName(runName)
        bundleDict[metricId, metricName] = newbundle
    return bundleDict
